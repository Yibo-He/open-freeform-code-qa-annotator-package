One possible solution to speed up reading multiple pickle files is to use the `multiprocessing.Pool` class from the `multiprocessing` module. This allows you to create a pool of worker processes that can execute tasks in parallel. Here's an example code snippet:

```python
import pickle
import multiprocessing

def read_pickle_file(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f)
    return data

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl']
    with multiprocessing.Pool() as pool:
        results = pool.map(read_pickle_file, filenames)
    # results is a list of the data loaded from each file
```

This code creates a pool of worker processes and uses the `map` method to apply the `read_pickle_file` function to each filename in parallel. The results are returned as a list in the same order as the input filenames.

Note that the `if __name__ == '__main__':` guard is necessary to prevent the worker processes from trying to execute the code inside the `if` block.

Another option is to use the `concurrent.futures` module, which provides a higher-level interface for parallel execution. Here's an example:

```python
import pickle
import concurrent.futures

def read_pickle_file(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f)
    return data

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl']
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(executor.map(read_pickle_file, filenames))
    # results is a list of the data loaded from each file
```

This code uses a `ThreadPoolExecutor` to execute the `read_pickle_file` function in multiple threads. The `map` method returns an iterator that yields the results as they become available, so we convert it to a list to get all the results at once. Note that we use a thread pool instead of a process pool because the GIL is not a problem when reading pickle files.