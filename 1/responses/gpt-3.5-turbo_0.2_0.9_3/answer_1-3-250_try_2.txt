One possible solution to speed up reading multiple pickle files is to use the `multiprocessing.Pool` class from the `multiprocessing` module. This allows you to parallelize the reading of the files across multiple processes, which can significantly reduce the overall time required.

Here's an example of how you could use `multiprocessing.Pool` to read a list of pickle files in parallel:

```python
import multiprocessing
import pandas as pd

def read_pickle_file(filename):
    return pd.read_pickle(filename)

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl', ...]

    with multiprocessing.Pool() as pool:
        data = pool.map(read_pickle_file, filenames)
```

In this example, we define a function `read_pickle_file` that takes a filename as input and returns the contents of the pickle file using `pd.read_pickle`. We then use `multiprocessing.Pool` to create a pool of worker processes and call `pool.map` with our `read_pickle_file` function and the list of filenames. This will execute the `read_pickle_file` function in parallel across multiple processes, with each process reading a different file.

Note that we use `if __name__ == '__main__':` to ensure that the code is only executed when the script is run as the main program, and not when it is imported as a module. This is necessary to avoid issues with pickling and multiprocessing.

I hope this helps! Let me know if you have any further questions.