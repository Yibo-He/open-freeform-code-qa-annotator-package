To implement the learning rate schedule suggested in the paper with the Adam optimizer in PyTorch, you can use the `LambdaLR` scheduler provided by PyTorch. Here's an example code snippet:

```python
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR

# Define your Adam optimizer with an initial learning rate
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Define the lambda function for the learning rate schedule
lr_lambda = lambda step: (d_model ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5)

# Create the LambdaLR scheduler
scheduler = LambdaLR(optimizer, lr_lambda)

# Train your model in a loop
for step in range(num_steps):
    # Perform a forward pass and compute loss
    loss = model.forward()

    # Backpropagation and optimization step
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Update the learning rate
    scheduler.step()
```

In the above code, `d_model` is the dimensionality of the model's hidden states, and `warmup_steps` is the number of warmup steps during which the learning rate is increased linearly. You can adjust these values according to your specific model and training setup.