The problem is that when you save the model, the custom lambda layer is saved without its context (i.e., the tensorflow library). When you load the model, it doesn't know what `tf` is, hence the error.

To solve this, you can define your lambda function as a standalone function and then use it in your model. This way, when you load the model, you can provide the function as a custom object.

Here is how you can do it:

First, define your lambda function as a standalone function:

```python
def reduce_mean(x):
    import tensorflow as tf
    return tf.reduce_mean(x, axis=1)
```

Then, use it in your model:

```python
model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))
model.add(Lambda(reduce_mean))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_val_pad, y_val), verbose=2)
model.save('my_model.h5')
```

Finally, when you load the model, provide the function as a custom object:

```python
from keras.models import load_model

def reduce_mean(x):
    import tensorflow as tf
    return tf.reduce_mean(x, axis=1)

def learning(test_samples):
    model = load_model('my_model.h5', custom_objects={'reduce_mean': reduce_mean})
    #rest of the code
```

This way, the model knows what `tf.reduce_mean` is when it loads the model.