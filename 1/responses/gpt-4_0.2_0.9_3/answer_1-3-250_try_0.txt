Given that you're working with `pandas.Series` objects, you might want to consider using Dask, a parallel computing library built with Python that integrates well with Pandas. Dask allows you to work on larger-than-memory datasets by breaking them down into smaller chunks and processing these chunks in parallel.

Here's a simple example of how you might use Dask to read multiple pickle files in parallel:

```python
import dask.dataframe as dd
from dask.diagnostics import ProgressBar

# Create a list of your pickle file paths
files = ['file1.pkl', 'file2.pkl', 'file3.pkl', ...]

# Use Dask to read the pickle files in parallel
df = dd.from_pandas(pd.concat([pd.read_pickle(f) for f in files]), npartitions=10)

with ProgressBar():
    df = df.compute()  # This will execute the task
```

In this example, Dask breaks the task of reading the pickle files into smaller tasks that it can execute in parallel. The `npartitions` argument to `dd.from_pandas` determines the number of partitions Dask should break the data into. You might need to adjust this depending on the size of your data and the number of cores on your machine.

Please note that Dask's parallel computations are performed in-memory and are not suitable for datasets that are larger than your machine's RAM. If you're working with very large datasets, you might need to consider other solutions, such as using a distributed computing framework like Apache Spark.