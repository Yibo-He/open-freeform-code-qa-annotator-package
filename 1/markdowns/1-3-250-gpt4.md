
# Post \#66348333 [Link](https://stackoverflow.com/questions/66348333/)

## Speed up reading multiple pickle files

**Vote**: 17 (290/702) **Views**: 3852 (548/702) 

**Internal ID** \#1-3-250

Created at 2021-02-24 09:46:41

Tags: `python` `python-multiprocessing` `python-multithreading`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I have a lot of pickle files. Currently I read them in a loop but it takes a lot of time. I would like to speed it up but don't have any idea how to do that.
Multiprocessing wouldn't work because in order to transfer data from a child subprocess to the main process data need to be serialized (pickled) and deserialized.
Using threading wouldn't help either because of GIL.
I think that the solution would be some library written in C that takes a list of files to read and then runs multiple threads (without GIL). Is there something like this around?

Answering your questions:
- - `pandas.Series`- - - - - 


----------
        
## GPT4 Attempt \#0

Given that you're working with `pandas.Series` objects, you might want to consider using Dask, a parallel computing library built with Python that integrates well with Pandas. Dask allows you to work on larger-than-memory datasets by breaking them down into smaller chunks and processing these chunks in parallel.

Here's a simple example of how you might use Dask to read multiple pickle files in parallel:

```python
import dask.dataframe as dd
from dask.diagnostics import ProgressBar

# Create a list of your pickle file paths
files = ['file1.pkl', 'file2.pkl', 'file3.pkl', ...]

# Use Dask to read the pickle files in parallel
df = dd.from_pandas(pd.concat([pd.read_pickle(f) for f in files]), npartitions=10)

with ProgressBar():
    df = df.compute()  # This will execute the task
```

In this example, Dask breaks the task of reading the pickle files into smaller tasks that it can execute in parallel. The `npartitions` argument to `dd.from_pandas` determines the number of partitions Dask should break the data into. You might need to adjust this depending on the size of your data and the number of cores on your machine.

Please note that Dask's parallel computations are performed in-memory and are not suitable for datasets that are larger than your machine's RAM. If you're working with very large datasets, you might need to consider other solutions, such as using a distributed computing framework like Apache Spark.

--------

## GPT4 Attempt \#1

If you are dealing with `pandas.Series` objects, you can use the `pandas.read_pickle` function to read the pickle files. However, if the reading process is slow due to the size of the files or the number of files, you might want to consider a few options:

1. **Dask**: Dask is a flexible library for parallel computing in Python that integrates with Pandas. It can handle large datasets that don't fit into memory and can use multiple cores. It's built on top of existing Python APIs and data structures, so you can move between Dask and Pandas/NumPy as appropriate.

2. **Ray**: Ray is a fast and simple framework for building and running distributed applications. Ray uses a global scheduler to achieve high performance and can be used with Pandas.

3. **Vaex**: Vaex is a python library for lazy, out-of-core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can be faster than Pandas, because it uses memory mapping, a method used to store data in memory that allows you to work with larger datasets.

4. **Modin**: Modin is another library that speeds up your Pandas workflows by changing a single line of code. It uses Ray or Dask to provide an effortless way to speed up your pandas notebooks, scripts, and libraries.

Remember, the best library for you depends on your specific use case and requirements. You might need to try a few of them to see which one works best for your needs.

--------

## GPT4 Attempt \#2

If you are dealing with `pandas.Series` objects, you can use the `pandas.read_pickle` function to read the pickle files. However, if the reading process is slow due to the size of the files or the number of files, you might want to consider a few options:

1. **Dask**: Dask is a flexible library for parallel computing in Python that integrates with Pandas. It can handle large datasets that don't fit into memory and can use multiple cores. It's built on top of existing Python APIs and data structures, so you can move between Dask and Pandas/NumPy as appropriate.

2. **Ray**: Ray is a fast and simple framework for building and running distributed applications. Ray uses a global scheduler to achieve high performance and can be used with Pandas.

3. **Vaex**: Vaex is a python library for lazy, out-of-core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can be faster than Pandas, because it uses memory mapping, a method used to store data in memory that allows you to work with larger datasets.

4. **Modin**: Modin is another library that speeds up your Pandas workflows by changing a single line of code. It uses Ray or Dask to provide an effortless way to speed up your pandas notebooks, scripts, and libraries.

Remember, the best library for you depends on your specific use case and requirements. You might need to try a few of them to see which one works best for your needs.

--------
