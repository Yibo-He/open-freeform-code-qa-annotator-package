
# Post \#70240506 [Link](https://stackoverflow.com/questions/70240506/)

## Why is numpy native on M1 Max greatly slower than on old Intel i5?

**Vote**: 14 (336/702) **Views**: 10662 (410/702) 

**Internal ID** \#1-3-203

Created at 2021-12-06 03:16:05

Tags: `python` `numpy` `tensorflow` `anaconda` `apple-m1`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I just got my new MacBook Pro with M1 Max chip and am setting up Python. I've tried several combinational settings to test speed - now I'm quite confused. First put my questions here:
- - - - 
Evidence supporting my questions is as follows:

---


Here are the settings I've tried:

- [Miniforge-arm64](https://github.com/conda-forge/miniforge)`Kind``Apple`- [Anaconda](https://docs.anaconda.com/anaconda/install/mac-os/)`Kind``Intel`

- `conda install numpy`- 
```
conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
```


- - [Apple Silicon version](https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=macM1)

---


Here is the test code:
```
import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')
```

and here are the results:
```
+-----------------------------------+-----------------------+--------------------+
|   Python installed by (run on)→   | Miniforge (native M1) | Anaconda (Rosseta) |
+----------------------+------------+------------+----------+----------+---------+
| Numpy installed by ↓ | Run from → |  Terminal  |  PyCharm | Terminal | PyCharm |
+----------------------+------------+------------+----------+----------+---------+
|          Apple Tensorflow         |   4.19151  |  4.86248 |     /    |    /    |
+-----------------------------------+------------+----------+----------+---------+
|        conda install numpy        |   4.29386  |  4.98370 |  4.10029 | 4.99271 |
+-----------------------------------+------------+----------+----------+---------+
```

This is quite slow. For comparison,
- `2.39917s`- [post (but not in English)](https://www.zhihu.com/question/431073191/answer/1657202944)`2.53214s``1.00613s`- 
Here is the CPU information details:
- 
```
$ sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Intel(R) Core(TM) i5-6360U CPU @ 2.00GHz
machdep.cpu.core_count: 2
```

- 
```
% sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Apple M1 Max
machdep.cpu.core_count: 10
```


---


I follow instructions strictly from tutorials - but why would all these happen? Is it because of my installation flaws, or because of M1 Max chip? Since my work relies heavily on local runs, local speed is very important to me. Any suggestions to possible solution, or any data points on your own device would be greatly appreciated :)


----------
        
## GPT4 Attempt \#0

The M1 Max chip is Apple's own silicon, which uses the ARM64 architecture, different from the x86_64 architecture that Intel chips use. This difference in architecture can cause some compatibility issues with software that was originally designed for Intel chips, including Python and its packages.

From your test results, it seems that running Python and numpy natively on the M1 Max chip (Miniforge) is slightly faster than running them through Rosetta 2 (Anaconda), which is Apple's technology for translating software written for Intel chips to work on their own silicon.

However, the performance difference between running the code in Terminal and PyCharm is negligible, which suggests that the IDE doesn't significantly affect the speed of the code.

The comparison results you mentioned are significantly faster, but it's not clear what setup they used. It's possible that they used a different version of Python or numpy, or a different method of installing them, or even a different machine with a faster CPU.

Here are some suggestions to improve the speed of your Python code on the M1 Max:

1. Use the latest version of Python and numpy: The latest versions often have performance improvements and bug fixes. You can check your Python version with `python --version` and your numpy version with `import numpy; print(numpy.__version__)` in Python.

2. Use native ARM64 versions of Python and its packages: Running software natively on the M1 Max chip is generally faster than running it through Rosetta 2. You can install the ARM64 version of Python through Miniforge, and install numpy with `conda install numpy`.

3. Use optimized libraries: Some libraries, like TensorFlow, have versions optimized for Apple's silicon. You can install them with `conda install -c apple tensorflow-deps; python -m pip install tensorflow-macos; python -m pip install tensorflow-metal`.

4. Use a profiler to optimize your code: A profiler can help you find the parts of your code that are taking the most time, so you can focus on optimizing them. Python's built-in cProfile module can be a good starting point.

5. Consider using a GPU or other hardware accelerators: If your work involves a lot of numerical computations, like machine learning, using a GPU or other hardware accelerators can significantly speed up your code. However, this may require changing your code to use libraries that support these accelerators, like TensorFlow or PyTorch.

6. Consider using cloud computing: If local speed is not sufficient, you can consider using cloud computing services, like AWS or Google Cloud, which can provide more powerful machines for running your code. However, this can increase costs.

Remember, the performance can vary depending on the specific code and setup, so it's a good idea to do your own benchmarks to find the best setup for your needs.

--------

## GPT4 Attempt \#1

The performance difference you're observing is likely due to a combination of factors:

1. **Native vs Rosetta 2**: Running Python and its libraries natively on the M1 chip should generally be faster than running them under Rosetta 2, which is a translation layer for running Intel-based apps on Apple Silicon. However, not all Python libraries have been optimized for the M1 chip yet, which might explain why you're not seeing a significant speedup.

2. **Python interpreter**: The Python interpreter you're using can also affect performance. For example, PyPy is often faster than CPython for long-running programs, but might be slower for short scripts due to its JIT compilation overhead.

3. **Python libraries**: The performance of Python libraries can vary greatly depending on how they're implemented and whether they're using native code or not. For example, NumPy and TensorFlow have parts that are written in C and Fortran for performance reasons, but if these parts aren't optimized for the M1 chip, they might run slower than expected.

4. **System load**: The performance can also be affected by the overall system load. If your MacBook Pro is running other heavy tasks in the background, it might slow down your Python scripts.

Here are some suggestions to improve the performance:

1. **Use native Python and libraries**: Try to use a Python distribution and libraries that are native to the M1 chip. For example, you can use the Miniforge distribution and install libraries from the conda-forge channel, which provides ARM64 versions of many popular libraries.

2. **Use optimized libraries**: Some libraries have optimized versions for specific hardware. For example, TensorFlow has a version optimized for Apple's M1 chip called TensorFlow Mac OS, which you can install with `pip install tensorflow-macos`.

3. **Use a faster Python interpreter**: If your scripts are long-running, you might benefit from using a faster Python interpreter like PyPy. However, keep in mind that not all Python libraries are compatible with PyPy.

4. **Reduce system load**: Try to reduce the overall system load by closing unnecessary apps and background tasks.

5. **Update your software**: Make sure your macOS, Python, and libraries are up-to-date. Newer versions often come with performance improvements and bug fixes.

Remember that performance tuning is often a process of trial and error, so don't be discouraged if you don't see immediate results. Keep experimenting with different settings and configurations until you find what works best for your specific use case.

--------

## GPT4 Attempt \#2

The performance difference you're seeing is likely due to a combination of factors, including the M1 Max chip, the Python distribution you're using, and how you're running your code.

1. **M1 Max Chip**: The M1 Max chip is Apple's own ARM-based processor, and it's quite different from the Intel processors that most Python packages have been optimized for. This means that some packages may not run as efficiently on the M1 Max as they would on an Intel processor. However, many popular Python packages, including NumPy and TensorFlow, have been optimized for the M1 chip, so you should still be able to get good performance.

2. **Python Distribution**: The Python distribution you're using can also affect performance. Anaconda, for example, includes a lot of pre-compiled binary packages that are optimized for speed. However, these packages are mostly compiled for Intel processors, so they may not run as efficiently on the M1 Max. Miniforge, on the other hand, is a smaller distribution that's designed to be compatible with a wide range of systems, including the M1 Max. It doesn't include as many pre-compiled packages as Anaconda, but it should still provide good performance.

3. **Running Code**: How you're running your code can also affect performance. Running code from the terminal can be faster than running it from an IDE like PyCharm, because the IDE has to do a lot of extra work to manage the code and provide features like syntax highlighting and auto-completion. However, the difference in performance should not be too significant.

Here are some suggestions to improve your performance:

1. **Use Native M1 Packages**: Try to use packages that have been optimized for the M1 Max chip whenever possible. For example, you can install the ARM64 version of Miniforge, and then use it to install NumPy and other packages.

2. **Use a Lightweight IDE or Text Editor**: If you're finding that PyCharm is slowing down your code, you might want to try a more lightweight IDE or text editor. VSCode, for example, is a popular choice among Python developers.

3. **Optimize Your Code**: There may be ways to optimize your code to make it run faster. For example, you could use NumPy's built-in functions instead of writing your own loops, or use a library like Numba to speed up your calculations.

4. **Consider Using Cloud-Based Solutions**: If local speed is a concern, you might want to consider using a cloud-based solution like Google Colab or AWS SageMaker. These platforms can provide you with access to powerful computing resources, and they can be a good option if you're working with large datasets or complex models.

--------
