
# Post \#65686318 [Link](https://stackoverflow.com/questions/65686318/)

## Sharing python objects across multiple workers

**Vote**: 27 (213/702) **Views**: 17597 (325/702) 

**Internal ID** \#1-3-258

Created at 2021-01-12 14:51:10

Tags: `python` `asynchronous` `python-asyncio` `fastapi`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

We have created a service using FastAPI. When our service starts it creates a few python objects that the endpoints then use to store or retrieve data from.
FastAPI in production starts with multiple workers. Our problem is that .
The script below shows a (simplified) example of what we are doing, though in our case the usage of Meta() is considerably more complex.
```
from fastapi import FastAPI, status

class Meta:
   def __init__(self):
      self.count = 0  

app = FastAPI()

meta = Meta()

# increases the count variable in the meta object by 1
@app.get("/increment")
async def increment():
    meta.count += 1
    return status.HTTP_200_OK

# returns a json containing the current count from the meta object
@app.get("/report")
async def report():
    return {'count':meta.count}

# resets the count in the meta object to 0
@app.get("/reset")
async def reset():
    meta.count = 0
    return status.HTTP_200_OK
```

As mentioned above, the problem with multiple workers is that each one will have its own `meta` object. Please be aware that the issue is not visible when running the api with a single worker.
More explicitly, when we hit the `/increment` endpoint for the first time we will see only one of the two workers responding to the call (this is correct, we don't want both workers doing the same thing). However, because there are two separate `meta` objects, only one of the two will be incremented.  When hitting the `/report` endpoint, depending on which worker responds to the request, either 1 or 0 will be returned.
The question then is, how do we get the workers to share and operate on the same object?
As a side question, the problem above affects the `/reset` endpoint too. If this endpoint is called then only one of the workers will reset its object. Is there a way to force all workers to respond to a single call on an endpoint?
Thanks!
Edit: I forgot to mention that we have tried (with no success) to store the `meta` object in the `app.state` instead. Essentially:
```
app.state.meta = Meta()
...
@app.get("/report")
async def report():
    return {'count':app.state.meta.count}
```



----------
        
## GPT3.5 Attempt \#0

To solve the problem of multiple workers having their own `meta` object, you can use a shared resource like a database or a cache to store the `meta` object. This way, all workers can access and modify the same object.

For example, you can use a cache like Redis to store the `meta` object. Here's an example of how you can modify your code to use Redis:

```
from fastapi import FastAPI, status
import redis

class Meta:
   def __init__(self, count):
      self.count = count

app = FastAPI()

redis_client = redis.Redis(host='localhost', port=6379, db=0)

# initialize the meta object in Redis
if not redis_client.exists('meta'):
    meta = Meta(0)
    redis_client.set('meta', pickle.dumps(meta))

# increases the count variable in the meta object by 1
@app.get("/increment")
async def increment():
    meta = pickle.loads(redis_client.get('meta'))
    meta.count += 1
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK

# returns a json containing the current count from the meta object
@app.get("/report")
async def report():
    meta = pickle.loads(redis_client.get('meta'))
    return {'count':meta.count}

# resets the count in the meta object to 0
@app.get("/reset")
async def reset():
    meta = Meta(0)
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK
```

In this example, we're using the `redis` library to connect to a Redis instance running on `localhost` and port `6379`. We're also using the `pickle` module to serialize and deserialize the `Meta` object.

When the service starts, we check if the `meta` object exists in Redis. If it doesn't, we create a new `Meta` object with a count of 0 and store it in Redis.

In the `increment` endpoint, we load the `meta` object from Redis, increment the count, and store it back in Redis.

In the `report` endpoint, we load the `meta` object from Redis and return the count.

In the `reset` endpoint, we create a new `Meta` object with a count of 0 and store it in Redis.

As for your side question, there's no way to force all workers to respond to a single call on an endpoint. Each worker is independent and handles requests separately. However, by using a shared resource like Redis, you can ensure that all workers are accessing and modifying the same data.

--------

## GPT3.5 Attempt \#1

To solve the problem of multiple workers having their own `meta` objects, you can use a shared memory solution such as Redis or Memcached. These tools allow you to store data in a shared memory space that can be accessed by multiple processes or workers.

Here's an example of how you can use Redis to share the `meta` object between workers:

1. Install the Redis library for Python:

```
pip install redis
```

2. Create a Redis client and use it to store the `meta` object:

```
import redis

redis_client = redis.Redis(host='localhost', port=6379, db=0)
redis_client.set('meta', pickle.dumps(Meta()))
```

3. In your endpoints, retrieve the `meta` object from Redis, modify it, and store it back:

```
@app.get("/increment")
async def increment():
    meta = pickle.loads(redis_client.get('meta'))
    meta.count += 1
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK

@app.get("/report")
async def report():
    meta = pickle.loads(redis_client.get('meta'))
    return {'count': meta.count}

@app.get("/reset")
async def reset():
    meta = pickle.loads(redis_client.get('meta'))
    meta.count = 0
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK
```

This way, all workers will be able to access and modify the same `meta` object stored in Redis.

As for your side question, there's no built-in way to force all workers to respond to a single call on an endpoint. However, you can use a similar approach as above to store a flag in Redis that indicates whether a reset has been requested, and have each worker check this flag periodically. If the flag is set, the worker can reset its own `meta` object.

--------

## GPT3.5 Attempt \#2

To solve the problem of multiple workers having their own `meta` object, you can use a shared memory object like Redis or Memcached to store the `meta` object. This way, all workers can access and modify the same object.

Here's an example of how you can use Redis to store the `meta` object:

```
import redis
from fastapi import FastAPI, status

class Meta:
   def __init__(self, count):
      self.count = count  

app = FastAPI()

redis_client = redis.Redis(host='localhost', port=6379, db=0)

# initialize the meta object in Redis
if not redis_client.exists('meta'):
    meta = Meta(0)
    redis_client.set('meta', pickle.dumps(meta))

# increases the count variable in the meta object by 1
@app.get("/increment")
async def increment():
    meta = pickle.loads(redis_client.get('meta'))
    meta.count += 1
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK

# returns a json containing the current count from the meta object
@app.get("/report")
async def report():
    meta = pickle.loads(redis_client.get('meta'))
    return {'count':meta.count}

# resets the count in the meta object to 0
@app.get("/reset")
async def reset():
    meta = Meta(0)
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK
```

In this example, we use the `redis` Python package to connect to a Redis server running on `localhost` and port `6379`. We initialize the `meta` object in Redis if it doesn't exist yet. Then, in each endpoint, we load the `meta` object from Redis, modify it, and store it back in Redis.

As for your side question, there's no built-in way to force all workers to respond to a single call on an endpoint. However, you can use a distributed lock to ensure that only one worker can execute the reset operation at a time. Redis provides a distributed lock implementation that you can use. Here's an example:

```
import redis
from fastapi import FastAPI, status

class Meta:
   def __init__(self, count):
      self.count = count  

app = FastAPI()

redis_client = redis.Redis(host='localhost', port=6379, db=0)

# initialize the meta object in Redis
if not redis_client.exists('meta'):
    meta = Meta(0)
    redis_client.set('meta', pickle.dumps(meta))

# increases the count variable in the meta object by 1
@app.get("/increment")
async def increment():
    meta = pickle.loads(redis_client.get('meta'))
    meta.count += 1
    redis_client.set('meta', pickle.dumps(meta))
    return status.HTTP_200_OK

# returns a json containing the current count from the meta object
@app.get("/report")
async def report():
    meta = pickle.loads(redis_client.get('meta'))
    return {'count':meta.count}

# resets the count in the meta object to 0
@app.get("/reset")
async def reset():
    # acquire a distributed lock
    lock = redis_client.lock('reset_lock', timeout=10)
    if lock.acquire(blocking=False):
        meta = Meta(0)
        redis_client.set('meta', pickle.dumps(meta))
        lock.release()
        return status.HTTP_200_OK
    else:
        return status.HTTP_503_SERVICE_UNAVAILABLE
```

In this example, we use the `redis.lock` method to acquire a lock named `reset_lock`. If the lock is acquired successfully, we reset the `meta` object and release the lock. If the lock cannot be acquired (i.e., another worker already has the lock), we return a 503 Service Unavailable status code.

--------
