
# Post \#62895219 [Link](https://stackoverflow.com/questions/62895219/)

## Getting Error in airflow DAG unsupported operand type(s) for >>: 'list' and 'list'. Sequential and Parallel execution of tasks

**Vote**: 7 (449/702) **Views**: 8512 (446/702) 

**Internal ID** \#1-3-275

Created at 2020-07-14 12:28:22

Tags: `python` `airflow` `directed-acyclic-graphs` `airflow-scheduler`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I am new to Apache airflow and DAG. There are total 6 tasks in the DAG (task1, task2, task3, task4, task5, task6). But at the time of running the DAG we are getting the error below.

Below is my code for the DAG. Please help. I am new to airflow.
```
from airflow import DAG
from datetime import datetime
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator


default_args = {
    'owner': 'airflow',
    'depends_on_past': False
}

dag = DAG('DAG_FOR_TEST',default_args=default_args,schedule_interval=None,max_active_runs=3, start_date=datetime(2020, 7, 14)) 


#################### CREATE TASK #####################################   

task_1 = DatabricksSubmitRunOperator(
    task_id='task_1',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',
    libraries= [
        {
        'jar': 'dbfs:/task_1/task_1.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_1.driver.TestClass1',
        'parameters' : [
            '{{ dag_run.conf.json }}'       
        ]
    }
)



    
task_2 = DatabricksSubmitRunOperator(
    task_id='task_2',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',   
    libraries= [
        {
        'jar': 'dbfs:/task_2/task_2.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_2.driver.TestClass2',
        'parameters' : [
            '{{ dag_run.conf.json }}'                               
        ]
    }
)
    
task_3 = DatabricksSubmitRunOperator(
    task_id='task_3',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',   
    libraries= [
        {
        'jar': 'dbfs:/task_3/task_3.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_3.driver.TestClass3',
        'parameters' : [
            '{{ dag_run.conf.json }}'   
        ]
    }
) 

task_4 = DatabricksSubmitRunOperator(
    task_id='task_4',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',
    libraries= [
        {
        'jar': 'dbfs:/task_4/task_4.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_4.driver.TestClass4',
        'parameters' : [
            '{{ dag_run.conf.json }}'   
        ]
    }
) 

task_5 = DatabricksSubmitRunOperator(
    task_id='task_5',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',
    libraries= [
        {
        'jar': 'dbfs:/task_5/task_5.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_5.driver.TestClass5',
        'parameters' : [
            'json ={{ dag_run.conf.json }}' 
        ]
    }
) 

task_6 = DatabricksSubmitRunOperator(
    task_id='task_6',
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',
    libraries= [
        {
        'jar': 'dbfs:/task_6/task_6.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_6.driver.TestClass6',
        'parameters' : ['{{ dag_run.conf.json }}'   
        ]
    }
) 

#################### ORDER OF OPERATORS ###########################  
 
    task_1.dag = dag
    task_2.dag = dag
    task_3.dag = dag
    task_4.dag = dag
    task_5.dag = dag
    task_6.dag = dag

task_1 >> [task_2 , task_3] >> [ task_4 , task_5 ] >> task_6
```



----------
        
## GPT4 Attempt \#0

From your code, it seems like you're trying to create a DAG with 6 tasks and you're using the DatabricksSubmitRunOperator to submit Spark jobs to Databricks. However, you didn't provide the error message you're encountering, which makes it difficult to provide a specific solution.

Here are some general tips to debug your DAG:

1. **Check your Airflow and Databricks Connection**: Make sure that the `databricks_conn_id` you're using is correctly set up in Airflow and it has the right permissions to submit jobs to Databricks.

2. **Check your Cluster ID**: You're using `{{ dag_run.conf.clusterId }}` as your `existing_cluster_id`. Make sure that you're passing the correct cluster ID when triggering the DAG run.

3. **Check your JAR files and Main Class Names**: Make sure that the JAR files you're referencing in `dbfs:/task_x/task_x.jar` exist in your Databricks File System (DBFS) and the main class names `com.task_x.driver.TestClassx` exist in your JAR files.

4. **Check your Task Dependencies**: You're setting up task dependencies at the end of your script with `task_1 >> [task_2 , task_3] >> [ task_4 , task_5 ] >> task_6`. Make sure that this is the correct order of execution for your tasks.

5. **Check your Airflow Logs**: Airflow logs can provide more details about the error. You can find them in the Airflow webserver under the "Log" tab of each task instance.

6. **Check your Databricks Logs**: If your tasks are failing on Databricks, you can check the logs in the Databricks workspace for more details about the error.

If you can provide the error message you're getting, I can give you a more specific solution.

--------

## GPT4 Attempt \#1

From the code you've shared, it seems like you're trying to create a Directed Acyclic Graph (DAG) with 6 tasks in Apache Airflow. The tasks are supposed to run in a specific order, with some tasks running in parallel.

However, you didn't provide the error message you're encountering, which makes it difficult to diagnose the problem. Here are a few common issues that might be causing your error:

1. **Incorrect Connection ID**: Ensure that the `databricks_conn_id` you're using is correctly set up in your Airflow connections.

2. **Incorrect Cluster ID**: The `existing_cluster_id` is being fetched from the DAG run configuration. Make sure that the `clusterId` is correctly specified in the configuration when triggering the DAG run.

3. **Incorrect JAR paths or Main Class Names**: The paths to the JAR files and the main class names specified in the `spark_jar_task` dictionary should be correct. If there's a typo or if the files don't exist at the specified paths, it would lead to an error.

4. **Incorrect DAG Definition**: The way you're setting the DAG for each task and defining the order of tasks is unconventional. Usually, you would pass the `dag` argument when creating the task, like so:

```python
task_1 = DatabricksSubmitRunOperator(
    task_id='task_1',
    dag=dag,
    # other arguments...
)
```

And you would define the order of tasks using the `>>` and `<<` operators, like so:

```python
task_1 >> task_2 >> task_3 >> task_4 >> task_5 >> task_6
```

If you could provide the error message you're seeing, I could give a more specific solution.

--------

## GPT4 Attempt \#2

From your code, it seems like you are trying to set the DAG for each task manually by using `task_1.dag = dag`, `task_2.dag = dag`, etc. This is not necessary because when you define the tasks, you can pass the `dag` argument directly to the `DatabricksSubmitRunOperator` like this:

```python
task_1 = DatabricksSubmitRunOperator(
    task_id='task_1',
    dag=dag,
    # other parameters...
)
```

Do this for all your tasks.

Also, you have a syntax error in the parameters for `task_5`. You have `'json ={{ dag_run.conf.json }}'` but it should be `'{{ dag_run.conf.json }}'` like in the other tasks.

Here is your corrected code:

```python
from airflow import DAG
from datetime import datetime
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False
}

dag = DAG('DAG_FOR_TEST',default_args=default_args,schedule_interval=None,max_active_runs=3, start_date=datetime(2020, 7, 14)) 

#################### CREATE TASK #####################################   

task_1 = DatabricksSubmitRunOperator(
    task_id='task_1',
    dag=dag,
    databricks_conn_id='connection_id_details',
    existing_cluster_id='{{ dag_run.conf.clusterId }}',
    libraries= [
        {
        'jar': 'dbfs:/task_1/task_1.jar'
        }        
        ],
    spark_jar_task={
        'main_class_name': 'com.task_1.driver.TestClass1',
        'parameters' : [
            '{{ dag_run.conf.json }}'       
        ]
    }
)

# Do the same for the other tasks...

#################### ORDER OF OPERATORS ###########################  

task_1 >> [task_2 , task_3] >> [ task_4 , task_5 ] >> task_6
```

This should solve your problem. If you still encounter issues, please provide the error message you are getting.

--------
