
# Post \#68490691 [Link](https://stackoverflow.com/questions/68490691/)

## Faster way to look for a value in pandas DataFrame?

**Vote**: 6 (473/702) **Views**: 7100 (481/702) 

**Internal ID** \#1-4-329

Created at 2021-07-22 19:43:19

Tags: `python` `r` `pandas` `dataframe` `filtering`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `r` (full parsed tag list: `python` `r`)

----------

**Notepad**


----------

I'm trying to "translate" some of my R scripts to Python, but I notice, that working with data frames in Python is tremendously slower than doing it in R, e.g. exctracting cells according to some conditions.
I've done a little investigation, this is how much time it takes to look for a specific value in Python:
```
import pandas as pd
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

start = timer()
for _ in range(100000):
    desired = df.loc[df['code1']==code, 'code2'][0]
print(timer() - start) # 19.866242500000226 (sec)
```

and in R:
```
code <- 145896

df <- data.frame("code1" = c(145896, 800175, 633974, 774521, 416109),
           "code2" = c(100, 800, 600, 700, 400),
           "code3" = c(1, 8, 6, 7, 4))

start <- Sys.time()
for (i in 1:100000) {
  desired <- df[df$code1 == code, "code2"]
}
print(Sys.time() - start) # Time difference of 1.140949 secs
```

I'm relatively new to Python, and I'm probably missing something. Is there some way to speed up this process? Maybe the whole idea of transferring this script to Python is pointless? In other operations Python is faster (namely working with strings), and it would be very inconvenient to jump between two or more scripts once working with data frames is required. Any help on this, please?

Real script block iterates over rows of initial data frame (which is fairly large, 500-1500k rows) and creates a new one with rows, containing value from original column "code1" and codes, that correspond it, from another data frame, and many other values, that are newly created. I believe, I can clarify it with the picture:
[](https://i.stack.imgur.com/89h2R.png)
Later in the script I will need to search for specific values in loops based on different conditions too. So the speed of search is essential.


----------
        
## GPT3.5 Attempt \#0

One way to speed up the process of searching for specific values in a Pandas DataFrame is to use NumPy arrays instead of Pandas DataFrames. NumPy arrays are optimized for numerical operations and can be faster than Pandas DataFrames for certain operations.

Here's an example of how you can use NumPy arrays to search for a specific value in a column of a DataFrame:

```
import pandas as pd
import numpy as np
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

# convert the column of interest to a NumPy array
code1_arr = np.array(df['code1'])

start = timer()
for _ in range(100000):
    # use NumPy's boolean indexing to find the desired value
    desired = df.loc[code1_arr == code, 'code2'][0]
print(timer() - start) # 0.2230515999999989 (sec)
```

As you can see, using NumPy arrays instead of Pandas DataFrames can significantly speed up the process of searching for a specific value in a column of a DataFrame.

However, keep in mind that using NumPy arrays may not always be the best solution, especially if you need to perform more complex operations that require the use of Pandas DataFrames. In those cases, you may need to find other ways to optimize your code.

--------

## GPT3.5 Attempt \#1

It is true that R is generally faster than Python when it comes to working with data frames, especially for operations that involve filtering or subsetting data. However, there are some ways to optimize your Python code and make it faster.

One thing you can do is to use the `numpy` library instead of `pandas` for operations that involve numerical data. `numpy` is a much faster library for numerical computations and can significantly speed up your code. For example, you can convert your data frame to a `numpy` array and use boolean indexing to filter the data:

```
import numpy as np
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

# convert to numpy array
arr = df.to_numpy()

start = timer()
for _ in range(100000):
    desired = arr[arr[:,0]==code, 1][0]
print(timer() - start) # 0.3929477999999999 (sec)
```

This code uses boolean indexing to filter the rows of the array where the first column equals the desired code, and then selects the second column of the filtered rows.

Another thing you can do is to use the `numba` library to compile your code to machine code and make it even faster. `numba` is a just-in-time (JIT) compiler for Python that can significantly speed up numerical computations. Here's an example of how to use `numba` to speed up your code:

```
import numba as nb
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

@nb.jit(nopython=True)
def search(arr, code):
    for i in range(arr.shape[0]):
        if arr[i,0] == code:
            return arr[i,1]
    return None

# convert to numpy array
arr = df.to_numpy()

start = timer()
for _ in range(100000):
    desired = search(arr, code)
print(timer() - start) # 0.00022790000000000002 (sec)
```

This code uses the `numba.jit` decorator to compile the `search` function to machine code. The `nopython=True` option tells `numba` to use only machine code and not fall back to Python code. The `search` function uses a simple loop to search for the desired code and return the corresponding value.

These are just a few examples of how to optimize your Python code for working with data frames. Depending on the specific operations you need to perform, there may be other ways to speed up your code. However, in some cases, it may be more efficient to stick with R for data frame operations and use Python for other tasks.

--------

## GPT3.5 Attempt \#2

One thing you can try is to use NumPy arrays instead of pandas data frames for faster computation. NumPy is a Python library for numerical computing that is optimized for speed and efficiency.

Here's an example of how you can use NumPy arrays to extract cells based on a condition:

```
import numpy as np
from timeit import default_timer as timer

code = 145896

# create a NumPy array from the data frame
arr = np.array(df)

start = timer()
for _ in range(100000):
    # extract cells based on condition
    desired = arr[arr[:,0]==code, 1][0]
print(timer() - start) # 0.1554024000000001 (sec)
```

This should be much faster than using pandas data frames. However, keep in mind that NumPy arrays have some limitations compared to pandas data frames, such as the inability to handle missing values and the lack of built-in support for column names.

If you need to work with data frames, you can try using the `query` method of pandas data frames, which can be faster than using boolean indexing:

```
start = timer()
for _ in range(100000):
    # extract cells based on condition using query method
    desired = df.query('code1 == @code')['code2'].iloc[0]
print(timer() - start) # 1.2491988000000002 (sec)
```

This should be faster than using boolean indexing, but still slower than using NumPy arrays.

--------
