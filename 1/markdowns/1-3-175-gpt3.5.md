
# Post \#60279160 [Link](https://stackoverflow.com/questions/60279160/)

## Compare two dataframes Pyspark

**Vote**: 7 (449/702) **Views**: 68888 (127/702) 

**Internal ID** \#1-3-175

Created at 2020-02-18 10:50:32

Tags: `python` `dataframe` `apache-spark` `pyspark` `apache-spark-sql`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I'm trying to compare two data frames with have same number of columns i.e. 4 columns with id as key column in both data frames

```
df1 = spark.read.csv("/path/to/data1.csv")
df2 = spark.read.csv("/path/to/data2.csv")
```


Now I want to append new column to DF2 i.e. column_names which is the list of the columns with different values than df1

```
df2.withColumn("column_names",udf())
```




```
+------+---------+--------+------+
|   id | |name  | sal  | Address |
+------+---------+--------+------+
|     1|  ABC   | 5000 | US      |
|     2|  DEF   | 4000 | UK      |
|     3|  GHI   | 3000 | JPN     |
|     4|  JKL   | 4500 | CHN     |
+------+---------+--------+------+
```




```
+------+---------+--------+------+
|   id | |name  | sal  | Address |
+------+---------+--------+------+
|     1|  ABC   | 5000 | US      |
|     2|  DEF   | 4000 | CAN     |
|     3|  GHI   | 3500 | JPN     |
|     4|  JKL_M | 4800 | CHN     |
+------+---------+--------+------+
```


Now I want DF3



```
+------+---------+--------+------+--------------+
|   id | |name  | sal  | Address | column_names |
+------+---------+--------+------+--------------+
|     1|  ABC   | 5000 | US      |  []          |
|     2|  DEF   | 4000 | CAN     |  [address]   |
|     3|  GHI   | 3500 | JPN     |  [sal]       |
|     4|  JKL_M | 4800 | CHN     |  [name,sal]  |
+------+---------+--------+------+--------------+
```


I saw this SO question, [How to compare two dataframe and print columns that are different in scala](https://stackoverflow.com/questions/44338412/how-to-compare-two-dataframe-and-print-columns-that-are-different-in-scala). Tried that, however the result is different.

I'm thinking of going with a UDF function by passing row from each dataframe to udf and compare column by column and return column list. However for that both the data frames should be in sorted order so that same id rows will be sent to udf. Sorting is costly operation here. Any solution?


----------
        
## GPT3.5 Attempt \#0

One approach to solve this problem is to use the `exceptAll` method to find the rows that are different between the two data frames. Then, you can use the `array` and `when` functions to create a new column that contains the names of the columns with different values.

Here's an example code snippet that should achieve the desired result:

```
from pyspark.sql.functions import array, col, when

# Find the rows that are different between the two data frames
diff_df = df1.exceptAll(df2)

# Create a new column that contains the names of the columns with different values
column_names = array([when(col(c1) != col(c2), c1).otherwise(None) for c1, c2 in zip(df1.columns, df2.columns)])
diff_df = diff_df.withColumn("column_names", column_names)

# Join the original data frame with the new column
result_df = df2.join(diff_df.select("id", "column_names"), "id", "left_outer")
```

This code should produce a data frame `result_df` that has the desired `column_names` column. Note that the `left_outer` join is used to ensure that all rows from `df2` are included in the result, even if there are no differences with `df1`.

--------

## GPT3.5 Attempt \#1

One approach to solve this problem is to use the `exceptAll` method of data frames. This method returns all rows in `df2` that are not present in `df1`. We can use this method to find the rows that have different values in `df2` compared to `df1`. Then, we can iterate over these rows and compare the values column by column to find the columns that have different values. Here's the code to achieve this:

```
from pyspark.sql.functions import array, col, lit, udf
from pyspark.sql.types import ArrayType, StringType

# read the data frames
df1 = spark.read.csv("/path/to/data1.csv", header=True)
df2 = spark.read.csv("/path/to/data2.csv", header=True)

# find the rows with different values in df2 compared to df1
diff_rows = df2.exceptAll(df1)

# define a UDF to compare the values of two rows and return the column names with different values
def get_diff_cols(row1, row2):
    diff_cols = []
    for i in range(len(row1)):
        if row1[i] != row2[i]:
            diff_cols.append(df1.columns[i])
    return diff_cols

# register the UDF
udf_get_diff_cols = udf(get_diff_cols, ArrayType(StringType()))

# add the column_names column to df2
df3 = diff_rows.withColumn("column_names", udf_get_diff_cols(array([df1[col] for col in df1.columns]), array([df2[col] for col in df2.columns])))

# fill null values in column_names column with empty list
df3 = df3.withColumn("column_names", when(col("column_names").isNull(), array(lit("")).otherwise(col("column_names"))))

# join df1 and df3 to get the final result
final_df = df1.join(df3, ["id", "name", "sal", "Address"], "left_outer").select(df1.columns + [df3["column_names"]])

final_df.show()
```

This code should give you the desired output. Note that we are using the `exceptAll` method to find the rows with different values, which is more efficient than sorting the data frames.

--------

## GPT3.5 Attempt \#2

One approach to solve this problem is to use the `exceptAll` method of DataFrame API which returns a new DataFrame containing rows in `df2` that are not present in `df1`. We can then use the `columns` method to get the list of columns in the resulting DataFrame and add it as a new column to `df2`.

Here's the code to implement this approach:

```
from pyspark.sql.functions import array, col, lit

# Get the columns that are different in df2 compared to df1
diff_cols = df2.exceptAll(df1).columns

# Add the diff_cols as a new column to df2
df3 = df2.withColumn("column_names", array([lit(c) for c in diff_cols]))

# Replace empty arrays with null values
df3 = df3.withColumn("column_names", col("column_names").isNull(), lit(None))
```

This should give you the desired output DataFrame `df3`. Note that we are using the `array` function to create an array of column names and the `lit` function to convert each column name to a literal value. We are also replacing empty arrays with null values using the `isNull` and `lit` functions.

--------
