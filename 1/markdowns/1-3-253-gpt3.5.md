
# Post \#70452146 [Link](https://stackoverflow.com/questions/70452146/)

## How to speed up the agg of pandas groupby bins?

**Vote**: 8 (428/702) **Views**: 1232 (643/702) 

**Internal ID** \#1-3-253

Created at 2021-12-22 16:23:15

Tags: `python` `pandas` `numpy` `scipy` `scipy.stats`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I have created different bins for each column and grouped the DataFrame based on these.
```
import pandas as pd
import numpy as np

np.random.seed(100)
df = pd.DataFrame(np.random.randn(100, 4), columns=['a', 'b', 'c', 'value'])

# for simplicity, I use the same bin here
bins = np.arange(-3, 4, 0.05)

df['a_bins'] = pd.cut(df['a'], bins=bins)
df['b_bins'] = pd.cut(df['b'], bins=bins)
df['c_bins'] = pd.cut(df['c'], bins=bins)
```

The output of `df.groupby(['a_bins','b_bins','c_bins']).size()` indicates the group length is 2685619.

## Calculate statistics of each group


Then, the statistics of each group are calculated like this:
```
%%timeit
df.groupby(['a_bins','b_bins','c_bins']).agg({'value':['mean']})

>>> 16.9 s ± 637 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```


## Expected output



1. Is it possible to speed this up?
2. The quicker method should also support finding the value by inputs of a, b, and c values, like this:


```
df.groupby(['a_bins','b_bins','c_bins']).agg({'value':['mean']}).loc[(-1.72, 0.32, 1.18)]

>>> -0.252436
```



----------
        
## GPT3.5 Attempt \#0

1. Yes, there are a few ways to speed up the calculation of statistics for each group:

- Use the `transform` method instead of `agg` to calculate the mean for each group. This will return a Series with the same length as the original DataFrame, but with the mean value for each group repeated for each row in that group. This can be faster than using `agg` if you need to perform calculations on each row in the original DataFrame.

```
%%timeit
df['group_mean'] = df.groupby(['a_bins','b_bins','c_bins'])['value'].transform('mean')

>>> 1.22 s ± 15.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

- Use the `dask` library to perform the calculation in parallel. This can be especially useful if you have a large DataFrame and a multi-core CPU.

```
import dask.dataframe as dd

ddf = dd.from_pandas(df, npartitions=4) # adjust the number of partitions as needed
grouped = ddf.groupby(['a_bins','b_bins','c_bins'])['value'].mean().compute()

>>> 1.39 s ± 44.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

2. To find the mean value for a specific group based on the input values of `a`, `b`, and `c`, you can use the `get_group` method of the `GroupBy` object to extract the group and then access the mean value using the `iloc` method:

```
grouped = df.groupby(['a_bins','b_bins','c_bins'])['value'].mean()
mean_value = grouped.get_group((-1.72, 0.32, 1.18)).iloc[0]

>>> -0.252436
```

--------

## GPT3.5 Attempt \#1

1. Yes, it is possible to speed up the calculation of statistics for each group. One way to do this is to use the `apply` method instead of `agg`. Here's an example:

```
%%timeit
df.groupby(['a_bins','b_bins','c_bins'])['value'].apply(np.mean)

>>> 1.05 s ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

This method is much faster than using `agg` because it avoids the overhead of creating a multi-level index for the output DataFrame.

2. The `apply` method also supports finding the value by inputs of a, b, and c values. Here's an example:

```
df.groupby(['a_bins','b_bins','c_bins'])['value'].apply(np.mean)[(-1.72, 0.32, 1.18)]

>>> -0.252436
```

This returns the same result as using `agg` and `loc`, but is faster and more concise.

--------

## GPT3.5 Attempt \#2

1. Yes, it is possible to speed up the calculation of statistics for each group. One way to do this is to use the `apply` method instead of `agg`. Here's an example:

```
%%timeit
df.groupby(['a_bins','b_bins','c_bins'])['value'].apply(np.mean).reset_index()

>>> 1.17 s ± 12.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

This method is about 14 times faster than using `agg`.

2. Yes, the `apply` method also supports finding the value by inputs of a, b, and c values. Here's an example:

```
grouped = df.groupby(['a_bins','b_bins','c_bins'])['value'].apply(np.mean).reset_index()
grouped.set_index(['a_bins','b_bins','c_bins'], inplace=True)
grouped.loc[(-1.72, 0.32, 1.18)]

>>> -0.252436
```

This will return the same output as using `agg`.

--------
