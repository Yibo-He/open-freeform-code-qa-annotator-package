
# Post \#68490691 [Link](https://stackoverflow.com/questions/68490691/)

## Faster way to look for a value in pandas DataFrame?

**Vote**: 6 (473/702) **Views**: 7100 (481/702) 

**Internal ID** \#1-4-329

Created at 2021-07-22 19:43:19

Tags: `python` `r` `pandas` `dataframe` `filtering`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `r` (full parsed tag list: `python` `r`)

----------

**Notepad**


----------

I'm trying to "translate" some of my R scripts to Python, but I notice, that working with data frames in Python is tremendously slower than doing it in R, e.g. exctracting cells according to some conditions.
I've done a little investigation, this is how much time it takes to look for a specific value in Python:
```
import pandas as pd
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

start = timer()
for _ in range(100000):
    desired = df.loc[df['code1']==code, 'code2'][0]
print(timer() - start) # 19.866242500000226 (sec)
```

and in R:
```
code <- 145896

df <- data.frame("code1" = c(145896, 800175, 633974, 774521, 416109),
           "code2" = c(100, 800, 600, 700, 400),
           "code3" = c(1, 8, 6, 7, 4))

start <- Sys.time()
for (i in 1:100000) {
  desired <- df[df$code1 == code, "code2"]
}
print(Sys.time() - start) # Time difference of 1.140949 secs
```

I'm relatively new to Python, and I'm probably missing something. Is there some way to speed up this process? Maybe the whole idea of transferring this script to Python is pointless? In other operations Python is faster (namely working with strings), and it would be very inconvenient to jump between two or more scripts once working with data frames is required. Any help on this, please?

Real script block iterates over rows of initial data frame (which is fairly large, 500-1500k rows) and creates a new one with rows, containing value from original column "code1" and codes, that correspond it, from another data frame, and many other values, that are newly created. I believe, I can clarify it with the picture:
[](https://i.stack.imgur.com/89h2R.png)
Later in the script I will need to search for specific values in loops based on different conditions too. So the speed of search is essential.


----------
        
## GPT4 Attempt \#0

In Python, the pandas library is designed to work with data frames, but it's not always the fastest option for certain operations. In your case, you're doing a lot of row-wise operations, which can be slow in pandas. 

One way to speed up your code is to use vectorized operations instead of loops. Vectorized operations are performed on entire arrays, which can be much faster than performing an operation for each individual element.

Here's an example of how you can use vectorized operations in your code:

```python
import pandas as pd
from timeit import default_timer as timer

code = 145896

df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

start = timer()
desired = df.loc[df['code1']==code, 'code2'].values
print(timer() - start) 
```

In this code, `df['code1']==code` creates a boolean mask of the same length as `df['code1']`, with `True` where the condition is met and `False` elsewhere. Then, `df.loc[mask, 'code2']` selects the 'code2' values where the mask is `True`. The `.values` attribute is used to get the result as a numpy array.

This should be much faster than your original code, because it avoids the use of a loop.

However, if you're doing a lot of complex operations with data frames, you might want to consider using a different library, such as Dask or Vaex, which are designed for large datasets and can be faster than pandas for certain operations.

Also, if you're doing a lot of operations that involve looking up values based on some condition, you might want to consider using a different data structure, such as a dictionary, which can be faster for this type of operation. However, this would involve a significant change to your code.

In general, the best approach depends on the specifics of your use case. If you're only doing a few operations with data frames, it might be easier to stick with pandas and try to optimize your code as much as possible. If you're doing a lot of complex operations with large data frames, it might be worth looking into other libraries or data structures.

--------

## GPT4 Attempt \#1

The performance difference you're seeing is due to the way you're using pandas. Pandas is not designed to be used in a row-wise manner, and doing so can lead to significant performance penalties. Instead, pandas is designed to be used with vectorized operations, which are operations that are performed on entire arrays at once. This is in contrast to R's data.frame, which is designed to be used in a row-wise manner.

Here's how you can rewrite your Python code to use vectorized operations:

```python
import pandas as pd
from timeit import default_timer as timer

code = 145896

# real df is way bigger
df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

start = timer()
desired = df.loc[df['code1']==code, 'code2'].values[0]
print(timer() - start) # significantly faster
```

In this code, `df.loc[df['code1']==code, 'code2'].values[0]` is a vectorized operation that performs the filtering operation on the entire array at once, rather than row by row.

However, if you need to perform this operation many times for different codes, it would be more efficient to set 'code1' as the index of the DataFrame and then use the `loc` function to access the rows. This is because the `loc` function can be much faster when using an index.

```python
import pandas as pd
from timeit import default_timer as timer

codes = [145896, 800175, 633974, 774521, 416109]

# real df is way bigger
df = pd.DataFrame(data={
    'code1': codes,
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    ).set_index('code1')

start = timer()
for code in codes:
    desired = df.loc[code, 'code2']
print(timer() - start) # significantly faster
```

In this code, `df.set_index('code1')` sets 'code1' as the index of the DataFrame, and `df.loc[code, 'code2']` uses the `loc` function to access the row for the given code. This is much faster than the previous approach, especially when the DataFrame is large.

--------

## GPT4 Attempt \#2

Pandas is a powerful library for data manipulation in Python, but it can be slow if not used correctly. The slowness you're experiencing is due to the fact that you're using a loop to access DataFrame rows, which is not the most efficient way to do it in Pandas. 

Pandas is built on top of NumPy, and it is designed to work best with vectorized operations. This means that operations are performed on entire arrays at once, rather than on individual elements (which is what happens when you use a loop). 

In your case, you're trying to filter rows based on a condition. This can be done much more efficiently using boolean indexing, which is a form of vectorized operation. Here's how you can do it:

```python
import pandas as pd
from timeit import default_timer as timer

code = 145896

df = pd.DataFrame(data={
    'code1': [145896, 800175, 633974, 774521, 416109],
    'code2': [100, 800, 600, 700, 400],
    'code3': [1, 8, 6, 7, 4]}
    )

start = timer()
desired = df.loc[df['code1']==code, 'code2'].values[0]
print(timer() - start) 
```

This will be much faster than using a loop, because the filtering operation is performed on the entire array at once.

However, if you need to perform this operation many times for different codes, you might want to consider using a different data structure, such as a dictionary, which has constant time complexity for lookups.

```python
df_dict = df.set_index('code1')['code2'].to_dict()

start = timer()
for _ in range(100000):
    desired = df_dict[code]
print(timer() - start) 
```

This will be even faster, because dictionaries in Python are implemented as hash tables, which provide constant time complexity for lookups. 

In general, when working with Pandas, you should try to avoid loops as much as possible, and use vectorized operations instead. This is one of the key principles for achieving good performance with Pandas.

--------
