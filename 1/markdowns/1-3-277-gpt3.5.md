
# Post \#65343377 [Link](https://stackoverflow.com/questions/65343377/)

## Adam optimizer with warmup on PyTorch

**Vote**: 14 (336/702) **Views**: 25056 (263/702) 

**Internal ID** \#1-3-277

Created at 2020-12-17 15:12:56

Tags: `python` `machine-learning` `pytorch`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

In the paper [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), under section 5.3, the authors suggested to increase the learning rate linearly and then decrease proportionally to the inverse square root of steps.
[](https://i.stack.imgur.com/GQurA.png)
How do we implement this in PyTorch with [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer? Preferably without additional packages.


----------
        
## GPT3.5 Attempt \#0

To implement the learning rate schedule suggested in the paper with Adam optimizer in PyTorch, you can use the `LambdaLR` scheduler provided by PyTorch. Here's an example code snippet:

```python
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR

# Define your Adam optimizer with initial learning rate
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Define the lambda function for learning rate schedule
lr_lambda = lambda step: (1 + step / warmup_steps) * (model_size ** -0.5) / warmup_factor

# Define the scheduler with the lambda function
scheduler = LambdaLR(optimizer, lr_lambda)

# Train your model in a loop
for step in range(num_steps):
    # Perform forward and backward pass
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    # Update the learning rate
    scheduler.step()
```

In the above code, `lr_lambda` is the lambda function that implements the learning rate schedule suggested in the paper. `warmup_steps` and `warmup_factor` are hyperparameters that control the warmup phase of the learning rate schedule. `model_size` is the size of the model, which is used to scale the learning rate. Finally, `LambdaLR` is used to create a scheduler that updates the learning rate according to the lambda function.

--------

## GPT3.5 Attempt \#1

To implement the learning rate schedule suggested in the paper with the Adam optimizer in PyTorch, you can use the `LambdaLR` scheduler provided by PyTorch. Here's an example code snippet:

```python
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR

# Define your Adam optimizer with an initial learning rate
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Define the lambda function for the learning rate schedule
lr_lambda = lambda step: (d_model ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5)

# Create the LambdaLR scheduler
scheduler = LambdaLR(optimizer, lr_lambda)

# Train your model in a loop
for step in range(num_steps):
    # Perform a forward pass and compute loss
    loss = model.forward()

    # Backpropagation and optimization step
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Update the learning rate
    scheduler.step()
```

In the above code, `d_model` is the dimensionality of the model's hidden states, and `warmup_steps` is the number of warmup steps during which the learning rate is increased linearly. You can adjust these values according to your specific model and training setup.

--------

## GPT3.5 Attempt \#2

To implement the learning rate schedule suggested in the paper, you can use the `LambdaLR` scheduler provided by PyTorch. Here's an example code snippet:

```python
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR

# Define your Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Define the lambda function for the learning rate schedule
lr_lambda = lambda step: (d_model ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5)

# Define the scheduler
scheduler = LambdaLR(optimizer, lr_lambda)

# Train your model
for epoch in range(num_epochs):
    for step, (inputs, targets) in enumerate(train_loader):
        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Update the learning rate
        scheduler.step()
```

In the above code, `d_model` is the dimensionality of the model, and `warmup_steps` is the number of warm-up steps. You can adjust these values according to your specific model and training setup. The `LambdaLR` scheduler applies the `lr_lambda` function to the learning rate at each step, which implements the learning rate schedule suggested in the paper.

--------
