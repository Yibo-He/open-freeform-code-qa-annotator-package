
# Post \#65343377 [Link](https://stackoverflow.com/questions/65343377/)

## Adam optimizer with warmup on PyTorch

**Vote**: 14 (336/702) **Views**: 25056 (263/702) 

**Internal ID** \#1-3-277

Created at 2020-12-17 15:12:56

Tags: `python` `machine-learning` `pytorch`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

In the paper [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), under section 5.3, the authors suggested to increase the learning rate linearly and then decrease proportionally to the inverse square root of steps.
[](https://i.stack.imgur.com/GQurA.png)
How do we implement this in PyTorch with [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer? Preferably without additional packages.


----------
        
## GPT4 Attempt \#0

To implement the learning rate schedule described in the "Attention is All You Need" paper with the Adam optimizer in PyTorch, you can create a custom learning rate scheduler. Here is a simple example:

```python
import math
import torch
from torch.optim import Adam

class CustomLR:
    def __init__(self, model_size, factor=1, warmup=4000):
        self.model_size = model_size
        self.factor = factor
        self.warmup = warmup
        self.step_num = 0

    def step(self):
        self.step_num += 1
        return self.factor * (self.model_size ** (-0.5) * min(self.step_num ** (-0.5), self.step_num * self.warmup ** (-1.5)))

# Initialize your model
model = ...

# Initialize your optimizer
optimizer = Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)

# Initialize your learning rate scheduler
scheduler = CustomLR(model_size=512)

for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        for param_group in optimizer.param_groups:
            param_group['lr'] = scheduler.step()
        optimizer.step()
```

In this example, `CustomLR` is a custom learning rate scheduler that calculates the learning rate based on the current step number. The learning rate is updated before each optimization step by setting `param_group['lr']` to the value returned by `scheduler.step()`. 

Please note that the learning rate is initialized to 0 in the Adam optimizer, as it will be updated by the scheduler at each step. The model size, warmup steps, and other parameters can be adjusted according to your specific use case.

--------

## GPT4 Attempt \#1

You can implement this learning rate schedule manually in PyTorch. Here is a simple example of how you might do it:

```python
import math
import torch
from torch.optim import Adam

class CustomScheduler:
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.step_num = 0
        self.warmup_steps = warmup_steps
        self.d_model = d_model

    def step(self):
        self.step_num += 1
        lr = self.learning_rate()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

    def learning_rate(self):
        return self.d_model**-0.5 * min(self.step_num**-0.5, self.step_num * self.warmup_steps**-1.5)

# Initialize your model and Adam optimizer
model = ...
optimizer = Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)

# Initialize the learning rate scheduler
scheduler = CustomScheduler(optimizer, d_model=512)

# In your training loop
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        scheduler.step()  # Update the learning rate
```

In this example, `CustomScheduler` is a learning rate scheduler that implements the learning rate schedule described in the "Attention is All You Need" paper. The learning rate is updated after each step (i.e., after each batch of data is processed). The `learning_rate` method computes the learning rate according to the formula in the paper.

Note that the learning rate is initially set to 0 in the Adam optimizer, and the actual learning rate is controlled by the scheduler. The `betas` and `eps` parameters are set according to the paper.

This is a basic implementation and may need to be adjusted based on your specific needs. For example, you might want to add some checks to make sure the learning rate doesn't become too large or too small.

--------

## GPT4 Attempt \#2

You can implement this learning rate schedule manually in PyTorch. Here is a simple example of how you can do it:

```python
import torch
import math

class CustomLR:
    def __init__(self, model_size, factor=1, warmup=4000):
        self.model_size = model_size
        self.factor = factor
        self.warmup = warmup
        self.step_num = 0

    def step(self):
        self.step_num += 1
        return self.factor * (self.model_size ** (-0.5) * min(self.step_num ** (-0.5), self.step_num * self.warmup ** (-1.5)))

model_size = 512
optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
scheduler = CustomLR(model_size)

for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        lr = scheduler.step()
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        optimizer.step()
```

In this code, `CustomLR` is a learning rate scheduler that implements the formula from the paper. The learning rate is updated before each step of the optimizer. The initial learning rate is set to 0 because it will be updated by the scheduler before the first step.

Please note that this is a basic implementation and might need to be adjusted based on your specific use case. For example, you might want to move the learning rate update to a separate function if you want to change the learning rate more frequently.

--------
