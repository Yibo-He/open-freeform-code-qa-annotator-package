
# Post \#65686318 [Link](https://stackoverflow.com/questions/65686318/)

## Sharing python objects across multiple workers

**Vote**: 27 (213/702) **Views**: 17597 (325/702) 

**Internal ID** \#1-3-258

Created at 2021-01-12 14:51:10

Tags: `python` `asynchronous` `python-asyncio` `fastapi`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

We have created a service using FastAPI. When our service starts it creates a few python objects that the endpoints then use to store or retrieve data from.
FastAPI in production starts with multiple workers. Our problem is that .
The script below shows a (simplified) example of what we are doing, though in our case the usage of Meta() is considerably more complex.
```
from fastapi import FastAPI, status

class Meta:
   def __init__(self):
      self.count = 0  

app = FastAPI()

meta = Meta()

# increases the count variable in the meta object by 1
@app.get("/increment")
async def increment():
    meta.count += 1
    return status.HTTP_200_OK

# returns a json containing the current count from the meta object
@app.get("/report")
async def report():
    return {'count':meta.count}

# resets the count in the meta object to 0
@app.get("/reset")
async def reset():
    meta.count = 0
    return status.HTTP_200_OK
```

As mentioned above, the problem with multiple workers is that each one will have its own `meta` object. Please be aware that the issue is not visible when running the api with a single worker.
More explicitly, when we hit the `/increment` endpoint for the first time we will see only one of the two workers responding to the call (this is correct, we don't want both workers doing the same thing). However, because there are two separate `meta` objects, only one of the two will be incremented.  When hitting the `/report` endpoint, depending on which worker responds to the request, either 1 or 0 will be returned.
The question then is, how do we get the workers to share and operate on the same object?
As a side question, the problem above affects the `/reset` endpoint too. If this endpoint is called then only one of the workers will reset its object. Is there a way to force all workers to respond to a single call on an endpoint?
Thanks!
Edit: I forgot to mention that we have tried (with no success) to store the `meta` object in the `app.state` instead. Essentially:
```
app.state.meta = Meta()
...
@app.get("/report")
async def report():
    return {'count':app.state.meta.count}
```



----------
        
## GPT4 Attempt \#0

The problem you're facing is due to the fact that each worker in FastAPI runs in its own process and therefore has its own memory space. This means that they can't directly share Python objects with each other. 

To share data between workers, you'll need to use a shared storage that all workers can access. This could be a database, a cache like Redis, or a message queue like RabbitMQ, depending on your needs.

Here's an example of how you could modify your code to use Redis for shared storage:

```python
from fastapi import FastAPI, status
import redis

app = FastAPI()

r = redis.Redis(host='localhost', port=6379, db=0)
r.set('count', 0)

@app.get("/increment")
async def increment():
    r.incr('count')
    return status.HTTP_200_OK

@app.get("/report")
async def report():
    return {'count': int(r.get('count'))}

@app.get("/reset")
async def reset():
    r.set('count', 0)
    return status.HTTP_200_OK
```

In this example, all workers will be able to increment, retrieve, and reset the count because they're all interacting with the same Redis instance.

As for your side question, if you want all workers to perform an action in response to a single request, you'll need to use a message queue or similar. When a request comes in, you'd send a message to the queue, and each worker would listen for messages on the queue and perform the action when it receives the message. However, this is a more complex solution and may not be necessary depending on your needs.

--------

## GPT4 Attempt \#1

The problem you're facing is due to the fact that each worker process in FastAPI runs in its own memory space. Therefore, they cannot directly share Python objects with each other. This is a common issue when dealing with multi-process architectures.

One common solution to this problem is to use a shared storage that all processes can access. This could be a database, a key-value store like Redis, or even a file on disk. The choice of storage depends on your specific use case.

Here's an example of how you could modify your code to use Redis as a shared storage:

```python
from fastapi import FastAPI, status
import redis

app = FastAPI()

r = redis.Redis(host='localhost', port=6379, db=0)

# increases the count variable in the redis store by 1
@app.get("/increment")
async def increment():
    r.incr('count')
    return status.HTTP_200_OK

# returns a json containing the current count from the redis store
@app.get("/report")
async def report():
    count = r.get('count')
    return {'count': int(count) if count else 0}

# resets the count in the redis store to 0
@app.get("/reset")
async def reset():
    r.set('count', 0)
    return status.HTTP_200_OK
```

In this example, all worker processes will share the same 'count' value in the Redis store. When you increment the count, it will be incremented in the Redis store and will be visible to all worker processes. Similarly, when you reset the count, it will be reset in the Redis store and this will also be visible to all worker processes.

As for your side question, in a multi-process architecture, you cannot force all workers to respond to a single call on an endpoint. Each request is handled by a single worker process. However, by using a shared storage as shown above, you can ensure that all workers have a consistent view of the data.

--------

## GPT4 Attempt \#2

The problem you're facing is a common one when dealing with multiple worker processes. Each worker process in FastAPI (or any other ASGI server like Uvicorn or Hypercorn) runs in its own separate memory space. This means that they don't share variables or objects in memory. This is why each worker has its own separate instance of the `meta` object.

To share data between workers, you need to use a shared storage that all workers can access. This could be a database, a cache like Redis, or a message queue like RabbitMQ. 

Here's an example of how you could modify your code to use Redis as a shared counter storage:

```python
from fastapi import FastAPI, status
import aioredis

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    app.state.redis = await aioredis.create_redis_pool('redis://localhost')

@app.on_event("shutdown")
async def shutdown_event():
    app.state.redis.close()
    await app.state.redis.wait_closed()

@app.get("/increment")
async def increment():
    await app.state.redis.incr('count')
    return status.HTTP_200_OK

@app.get("/report")
async def report():
    count = await app.state.redis.get('count', encoding='utf-8')
    return {'count': count}

@app.get("/reset")
async def reset():
    await app.state.redis.set('count', 0)
    return status.HTTP_200_OK
```

In this example, Redis is used as a shared storage for the counter. The `incr` command atomically increments the counter, and the `get` command retrieves the current count. The `set` command is used to reset the counter to 0.

As for your side question, there's no built-in way to force all workers to respond to a single call on an endpoint. However, you could use a similar approach to the one above: when the `/reset` endpoint is called, it could send a message to a shared queue or pub/sub channel, and all workers could listen for messages on that channel and reset their state when they receive a message.

--------
