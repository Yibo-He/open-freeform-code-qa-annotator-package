
# Post \#72618859 [Link](https://stackoverflow.com/questions/72618859/)

## Run two nested for loops in parallel to create matrix

**Vote**: 2 (631/702) **Views**: 777 (678/702) 

**Internal ID** \#1-3-283

Created at 2022-06-14 14:36:17

Tags: `python` `multithreading` `numpy`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I've written a method that takes in an integer "n" and creates a square matrix where the values of each element are dictated by their respective i,j indices.
When I build a small matrix 30x30 it works just fine, but when I try to do something larger like 1000x1000 it takes very long. Is there any way that I can speed it up with multiprocessing?
```
def createMatrix(n):
    matrix = []
    for j in range(1,n+1):
        row = []
        for i in range(1,n+1):
            value = 1/(i+j-1)
            row.append(value)
        matrix.append(row)
    return np.array(matrix)
```



----------
        
## GPT4 Attempt \#0

Yes, you can definitely speed up your code by using multiprocessing. Python's multiprocessing module allows you to create multiple processes, each with its own Python interpreter, which can overcome the Global Interpreter Lock (GIL) and make full use of multiple processors on a machine.

Here's how you can modify your code to use multiprocessing:

```python
import numpy as np
from multiprocessing import Pool

def calculate_value(params):
    i, j = params
    return 1/(i+j-1)

def createMatrix(n):
    with Pool() as p:
        matrix = []
        for j in range(1,n+1):
            row_params = [(i, j) for i in range(1, n+1)]
            row = p.map(calculate_value, row_params)
            matrix.append(row)
    return np.array(matrix)
```

In this code, we use a Pool of worker processes. We map the `calculate_value` function over the parameters for each row. The `map` function automatically distributes the computations over the available processors.

However, please note that multiprocessing introduces some overhead, because data needs to be pickled and unpickled to be sent between the main process and the worker processes. Therefore, it is most effective when the amount of computation done in each function call is large compared to the amount of data that needs to be transferred.

In your case, the computation done in each function call is quite small (just a single division), so the overhead of multiprocessing might actually make the code slower. You might want to consider other ways of speeding up your code, such as using NumPy's vectorized operations. Here's how you can do it:

```python
def createMatrix(n):
    i = np.arange(1, n+1)
    j = np.arange(1, n+1).reshape(-1, 1)
    matrix = 1 / (i + j - 1)
    return matrix
```

This code creates two arrays `i` and `j`, and then uses broadcasting to perform the division on all pairs of `i` and `j` at once. This is likely to be much faster than either the original code or the multiprocessing version, because NumPy's vectorized operations are implemented in C and can take advantage of low-level optimizations.

--------

## GPT4 Attempt \#1

Yes, you can use multiprocessing to speed up your matrix creation. Python's multiprocessing module allows you to create multiple processes, each with its own Python interpreter, which can run independently of each other. This can be especially useful for CPU-bound tasks like yours.

Here's an example of how you can modify your function to use multiprocessing:

```python
from multiprocessing import Pool
import numpy as np

def calculate_value(args):
    i, j = args
    return 1/(i+j-1)

def createMatrix(n):
    with Pool() as p:
        matrix = []
        for j in range(1,n+1):
            row = p.map(calculate_value, [(i, j) for i in range(1, n+1)])
            matrix.append(row)
        return np.array(matrix)
```

In this code, we're using a Pool of worker processes. For each row, we're using the map function to apply the calculate_value function to each element of the row in parallel. The calculate_value function takes a tuple of (i, j) as its argument, so we need to pass a list of tuples to the map function.

Please note that this code will create a new Pool of processes for each row, which can be inefficient if n is large. If that's the case, you might want to create the Pool outside the createMatrix function and pass it as an argument, so that the same Pool is used for all rows.

Also, keep in mind that multiprocessing has some overhead because it needs to create new processes and communicate data between them. Therefore, it might not always speed up your code, especially for small inputs or for tasks that are not CPU-bound. It's always a good idea to test your code with and without multiprocessing to see if it actually improves performance.

--------

## GPT4 Attempt \#2

Yes, you can use multiprocessing to speed up the process. Here's how you can modify your function to use Python's multiprocessing module:

```python
import multiprocessing as mp
import numpy as np

def worker(args):
    i, n = args
    row = [1/(i+j-1) for j in range(1, n+1)]
    return row

def createMatrix(n):
    pool = mp.Pool(mp.cpu_count())
    matrix = pool.map(worker, [(i, n) for i in range(1, n+1)])
    pool.close()
    return np.array(matrix)
```

In this version of the function, the `worker` function is what each process will run. It takes a tuple of arguments, which includes the row index and the size of the matrix. The `createMatrix` function creates a pool of processes, then uses the `map` function to apply the `worker` function to each row index. The results are collected into a list, which is then converted to a numpy array.

This should speed up the creation of the matrix, as the work is being done in parallel. However, keep in mind that there is overhead associated with creating processes and communicating between them, so this might not be faster for small matrices. Also, the speedup will be limited by the number of cores on your machine.

Also, note that this will only work if the calculation of each row is independent of the others. If they are not, you will need to use a different approach.

--------
