
# Post \#66348333 [Link](https://stackoverflow.com/questions/66348333/)

## Speed up reading multiple pickle files

**Vote**: 17 (290/702) **Views**: 3852 (548/702) 

**Internal ID** \#1-3-250

Created at 2021-02-24 09:46:41

Tags: `python` `python-multiprocessing` `python-multithreading`

----------

#### Metadata:

Area: `Data Science & Machine Learning`

Language: `python` (full parsed tag list: `python`)

----------

**Notepad**


----------

I have a lot of pickle files. Currently I read them in a loop but it takes a lot of time. I would like to speed it up but don't have any idea how to do that.
Multiprocessing wouldn't work because in order to transfer data from a child subprocess to the main process data need to be serialized (pickled) and deserialized.
Using threading wouldn't help either because of GIL.
I think that the solution would be some library written in C that takes a list of files to read and then runs multiple threads (without GIL). Is there something like this around?

Answering your questions:
- - `pandas.Series`- - - - - 


----------
        
## GPT3.5 Attempt \#0

One possible solution to speed up reading multiple pickle files is to use the `pandas` library. You can create a list of file paths and then use the `pandas.read_pickle()` function to read them all into a single `pandas.Series` object. This will be faster than reading them in a loop because `pandas` uses optimized C code to read the files.

Here's an example code snippet:

```python
import pandas as pd

file_paths = ['file1.pkl', 'file2.pkl', 'file3.pkl']
series_list = []

for file_path in file_paths:
    series_list.append(pd.read_pickle(file_path))

combined_series = pd.concat(series_list)
```

This code will read all the pickle files in `file_paths` into a list of `pandas.Series` objects, and then combine them into a single `pandas.Series` object using the `pd.concat()` function. This should be faster than reading the files in a loop because `pandas` uses optimized C code to read the files.

--------

## GPT3.5 Attempt \#1

One possible solution to speed up reading multiple pickle files is to use the `multiprocessing.Pool` class from the `multiprocessing` module. This allows you to create a pool of worker processes that can execute tasks in parallel. Here's an example code snippet:

```python
import pickle
import multiprocessing

def read_pickle_file(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f)
    return data

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl']
    with multiprocessing.Pool() as pool:
        results = pool.map(read_pickle_file, filenames)
    # results is a list of the data loaded from each file
```

This code creates a pool of worker processes and uses the `map` method to apply the `read_pickle_file` function to each filename in parallel. The results are returned as a list in the same order as the input filenames.

Note that the `if __name__ == '__main__':` guard is necessary to prevent the worker processes from trying to execute the code inside the `if` block.

Another option is to use the `concurrent.futures` module, which provides a higher-level interface for parallel execution. Here's an example:

```python
import pickle
import concurrent.futures

def read_pickle_file(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f)
    return data

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl']
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(executor.map(read_pickle_file, filenames))
    # results is a list of the data loaded from each file
```

This code uses a `ThreadPoolExecutor` to execute the `read_pickle_file` function in multiple threads. The `map` method returns an iterator that yields the results as they become available, so we convert it to a list to get all the results at once. Note that we use a thread pool instead of a process pool because the GIL is not a problem when reading pickle files.

--------

## GPT3.5 Attempt \#2

One possible solution to speed up reading multiple pickle files is to use the `multiprocessing.Pool` class from the `multiprocessing` module. This allows you to parallelize the reading of the files across multiple processes, which can significantly reduce the overall time required.

Here's an example of how you could use `multiprocessing.Pool` to read a list of pickle files in parallel:

```python
import multiprocessing
import pandas as pd

def read_pickle_file(filename):
    return pd.read_pickle(filename)

if __name__ == '__main__':
    filenames = ['file1.pkl', 'file2.pkl', 'file3.pkl', ...]

    with multiprocessing.Pool() as pool:
        data = pool.map(read_pickle_file, filenames)
```

In this example, we define a function `read_pickle_file` that takes a filename as input and returns the contents of the pickle file using `pd.read_pickle`. We then use `multiprocessing.Pool` to create a pool of worker processes and call `pool.map` with our `read_pickle_file` function and the list of filenames. This will execute the `read_pickle_file` function in parallel across multiple processes, with each process reading a different file.

Note that we use `if __name__ == '__main__':` to ensure that the code is only executed when the script is run as the main program, and not when it is imported as a module. This is necessary to avoid issues with pickling and multiprocessing.

I hope this helps! Let me know if you have any further questions.

--------
